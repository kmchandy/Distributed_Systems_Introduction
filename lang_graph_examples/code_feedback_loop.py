import os
import ast
from openai import OpenAI
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from typing import TypedDict, Optional


# ---------------------------------------------
# Step 1: Set up OpenAI.
# ----------------------------------------------

# Load API key from .env file
load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')

# Ensure the API key is available
if not api_key:
    raise ValueError("No API key found. Please check your .env file.")

# Create an OpenAI client
client = OpenAI(api_key=api_key)
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)


# ---------------------------------------------
# Step 2: Define the state: a TypedDict.
# ----------------------------------------------

class State(TypedDict):
    # The user-specified coding task
    task: Optional[str]
    # Code generated by the LLM
    code: Optional[str]
    # yes if the user approves the code generated by the LLM, and no otherwise.
    code_ok: Optional[bool]
    # Test value for executing the code
    code_test_value: Optional[str]
    # Output generated by running the code with the test value.
    code_execution_output: Optional[str]
    # yes if the user approves the output of code execution, and no otherwise.
    code_output_ok: Optional[bool]


# ---------------------------------------------
# Step 3: Specify the functions that are executed
# by nodes in the graph.
# The functions return a dict where the keys are
# also keys of State.
# ----------------------------------------------


def get_task_function(state: State) -> dict:
    """Prompt the user to describe the coding task."""
    task = input("\n What coding task do you want? (e.g., reverse a string): ")
    return {"task": task}


def generate_code_function(state: State) -> dict:
    """Use the LLM to generate Python code for the task."""
    task = state["task"]
    prompt = f'''
    Write a Python function for this task: {task}. Only include the code.
    Do NOT include descriptions or anything other than the code.
    '''
    result = llm.invoke(prompt)
    code = result.content.strip()
    return {"code": code}


def approve_code_function(state: State) -> dict:
    """Ask the user if the generated code is acceptable."""
    print("\n Generated Code:\n")
    print(state["code"])
    # Get input from command line of user.
    user_input = input("\n Do you approve this code? (yes/no): ")
    # Normalize user input to lowercase for consistency
    user_input = user_input.strip().lower()
    return {"code_ok": user_input == "yes"}


def get_input_function(state: State) -> dict:
    """Prompt the user for input to be passed into the generated code."""
    # Get input from command line of user.
    user_input = input(
        "\nEnter an input for the code to run on (e.g., 'hello' or [3, 1, 2]): ")
    try:
        # Safely interpret literals like lists, numbers, strings
        # ast.literal_eval allows the user to input Python literals directly
        parsed_val = ast.literal_eval(user_input)
    except Exception:
        parsed_val = user_input  # Fallback to raw string
    return {"code_test_value": parsed_val}


def run_code_function(state: State) -> dict:
    """Dynamically execute the user-approved code with the provided input."""
    # local_env allows the function defined in the code to be executed
    # safely without changing the global environment.
    local_env = {}
    code = state["code"]
    input_val = state["code_test_value"]

    try:
        exec(code, {}, local_env)
        func = next((v for v in local_env.values() if callable(v)), None)
        if not func:
            raise ValueError("No callable function found in generated code.")

        output = func(input_val)
        print("\nOutput:", output)
        return {"code_execution_output": str(output)}
    except Exception as e:
        print("\nError during code execution:", str(e))
        return {"code_execution_output": f"Error: {str(e)}"}


def approve_result_function(state: State) -> dict:
    """Ask the user if the result is correct."""
    print("\nResult from code execution:", state["code_execution_output"])
    response = input("\n Is the result correct? (yes/no): ").strip().lower()
    return {"code_output_ok": response == "yes"}


# ---------------------------------------------
# Step 4: Build the graph
# The nodes of the graph are agents that execute
# functions that read and write the state.
# ----------------------------------------------

# 4.1 Create builder
builder = StateGraph(State)

# 4.2 Add nodes to the graph.
# Give a name to the node and specify the function
# that will be executed by the node.
builder.add_node("get_task_node", get_task_function)
builder.add_node("generate_code_node", generate_code_function)
builder.add_node("approve_code_node", approve_code_function)
builder.add_node("get_input_node", get_input_function)
builder.add_node("run_code_node", run_code_function)
builder.add_node("approve_result_node", approve_result_function)


# 4.3 Define the edges between nodes of the graph.
builder.add_edge("get_task_node", "generate_code_node")
builder.add_edge("generate_code_node", "approve_code_node")

builder.add_conditional_edges(
    "approve_code_node",
    lambda x: "get input to test code" if x["code_ok"] else "develop new code",
    {
        "get input to test code": "get_input_node",
        "develop new code": "generate_code_node"
    }
)

builder.add_edge("get_input_node", "run_code_node")
builder.add_edge("run_code_node", "approve_result_node")

builder.add_conditional_edges(
    "approve_result_node",
    lambda x: "finished" if x["code_output_ok"] else "generate_code_node",
    {
        "finished": END,
        "code_gen": "generate_code_node"
    }
)

# 4.4 Specify the entry and finish points of the graph.
builder.set_entry_point("get_task_node")
# The finish point is specified by END

# 4.5 Compile the graph
graph = builder.compile()

if __name__ == "__main__":
    print("\n=== Human-in-the-Loop Coding Agent ===")
    graph.invoke({})
